---
layout: default
title: {{ site.name }}
---

### Key facts:
* <b>Instructor:</b> Komal Teru
* Term: Summer 2019
* When: Monday - Friday, 1-4pm
* Where: 3rd Floor, 550 ave Sherbrooke, Montreal
* See the [schedule](schedule.html) for more details.

### Description
This course is designed to give a breadth of exposure to most popular deep neural network architectures. We would not go into too much mathematical detail and rather rely on hands-on emperical findings to motivate and justify different architectures and approaches. 

We begin with an introduction to multi layer perceptrons. We use this to study general concepts of ML pipeline: loss functions, optimization algorithms, etc. During this introduction, we use a toy dataset to setup end-to-end training/evaluation pipeline in PyTorch. This pipeline is used as boiler plate code for all the emperical studies done throughout the course. We then proceed to introduce what CNN, RNN and their variants are. We will use artifically generated toy datasets to motivate different approaches and architectures.

### Prerequisites
Introductory knowledge of statistics, optimization, and linear algebra is expected. This course assumes basic knowledge of programming in Python. Though knowledge of object oriented programming is not mandated to grasp the material, it would definitely enable you to explore material not explicitly covered in the course.

### List of topics (subject to minor changes)
- Multi layer pereceptron
- Regularization methods: l2, dropout, batchnorm
- Convolutional neural networks
- Recurrent neural networks and applications


